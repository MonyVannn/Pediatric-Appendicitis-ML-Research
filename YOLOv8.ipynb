{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLOv8 Classification â€” 70/15/15 Pipeline\n",
        "\n",
        "This notebook:\n",
        "- Discovers or uses a specified dataset root of two classes: **Appendicitis** and **No_Appendicitis**\n",
        "- Creates a **stratified 70/15/15** split (train/val/test) with a **unique** test set\n",
        "- (Optional) Applies augmentation **only to train** to balance classes\n",
        "- Trains YOLOv8 **classification** on `datasets_70_15_15`\n",
        "- Computes per-image probabilities for a chosen split and **sweeps a decision threshold**\n",
        "\n",
        "> Edit just the *Parameters* cell if needed. The rest should run top-to-bottom after a clean kernel restart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Parameters (edit me) ---\n",
        "DATASET_SOURCE_CANDIDATES = [\n",
        "    \"datasets_70_15_15\",\n",
        "    \"datasets_val_richer\",\n",
        "    \"datasets_balanced\",\n",
        "    \"datasets\",\n",
        "]\n",
        "CLASSES = [\"Appendicitis\", \"No_Appendicitis\"]\n",
        "NEW_ROOT = \"datasets_70_15_15\"\n",
        "RUN_NAME_BASE = \"cls_s_70_15_15\"\n",
        "IMG_SIZE = 224\n",
        "EPOCHS_STAGE1 = 20\n",
        "EPOCHS_STAGE2 = 15\n",
        "BATCH = 32\n",
        "DEVICE = 0\n",
        "SEED = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os, shutil, math, random, cv2, json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "plt.rcParams['figure.figsize'] = (6,5)\n",
        "\n",
        "def nrm(s): return s.lower().replace(\"_\",\"\").replace(\" \",\"\")\n",
        "ALLOWED = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
        "\n",
        "def has_images(p: Path):\n",
        "    return any(f.suffix.lower() in ALLOWED for f in p.glob(\"*\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discover dataset root and build `df` (all images + labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using SOURCE_ROOT: datasets_70_15_15 | already_split= True\n",
            "All images: 4734 | App: 2550 | No: 2184\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def discover_root(candidates, classes):\n",
        "    for r in candidates:\n",
        "        root = Path(r)\n",
        "        if not root.exists():\n",
        "            continue\n",
        "        # split case\n",
        "        for split in [\"train\",\"val\",\"test\"]:\n",
        "            d = root / split\n",
        "            if d.is_dir() and all((d/c).exists() and has_images(d/c) for c in classes):\n",
        "                return root, True\n",
        "        # flat case\n",
        "        if all((root/c).exists() and has_images(root/c) for c in classes):\n",
        "            return root, False\n",
        "    return None, False\n",
        "\n",
        "SOURCE_ROOT, IS_SPLIT = discover_root(DATASET_SOURCE_CANDIDATES, CLASSES)\n",
        "assert SOURCE_ROOT is not None, f\"Couldn't find a valid dataset root with classes {CLASSES}. Edit DATASET_SOURCE_CANDIDATES.\"\n",
        "print(\"Using SOURCE_ROOT:\", SOURCE_ROOT, \"| already_split=\", IS_SPLIT)\n",
        "\n",
        "rows = []\n",
        "if IS_SPLIT:\n",
        "    for split in [\"train\",\"val\",\"test\"]:\n",
        "        d = SOURCE_ROOT / split\n",
        "        if not d.is_dir(): \n",
        "            continue\n",
        "        for cls in CLASSES:\n",
        "            p = d / cls\n",
        "            if not p.is_dir(): continue\n",
        "            for f in p.glob(\"*\"):\n",
        "                if f.suffix.lower() in ALLOWED:\n",
        "                    label = 1 if nrm(cls).startswith(\"append\") else 0\n",
        "                    rows.append((str(f), label))\n",
        "else:\n",
        "    for cls in CLASSES:\n",
        "        p = SOURCE_ROOT / cls\n",
        "        for f in p.glob(\"*\"):\n",
        "            if f.suffix.lower() in ALLOWED:\n",
        "                label = 1 if nrm(cls).startswith(\"append\") else 0\n",
        "                rows.append((str(f), label))\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(rows, columns=[\"path\",\"label\"]).drop_duplicates(\"path\").reset_index(drop=True)\n",
        "print(\"All images:\", len(df), \"| App:\", int((df.label==1).sum()), \"| No:\", int((df.label==0).sum()))\n",
        "assert len(df)>0, \"No images found.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split 70/15/15 (stratified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: total=3313 | App=1785 | No=1528\n",
            "Val: total=710 | App=382 | No=328\n",
            "Test: total=711 | App=383 | No=328\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.30, stratify=df['label'], random_state=SEED\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.50, stratify=temp_df['label'], random_state=SEED\n",
        ")\n",
        "\n",
        "for name, d in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
        "    pos = int((d.label==1).sum()); neg = int((d.label==0).sum())\n",
        "    print(f\"{name}: total={len(d)} | App={pos} | No={neg}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Materialize YOLO folder tree (train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[warn] Your df['path'] includes files under NEW_ROOT. Youâ€™re re-splitting an already-materialized tree. Weâ€™ll skip identical paths and existing files to avoid SameFileError.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[train] linked=25 copied=0 skipped_exist=1557 skipped_same=1731\n",
            "[val] linked=273 copied=0 skipped_exist=259 skipped_same=178\n",
            "[test] linked=296 copied=0 skipped_exist=250 skipped_same=165\n",
            "Built (or verified): /home/liori/appendicitis_project/datasets_70_15_15\n"
          ]
        }
      ],
      "source": [
        "# === Materialize YOLO folder tree (train/val/test) â€” idempotent & safe ===\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "CLASSES = [\"Appendicitis\", \"No_Appendicitis\"]\n",
        "NEW_ROOT = Path(\"datasets_70_15_15\")\n",
        "\n",
        "# warn if the source files are already inside NEW_ROOT (self-link risk)\n",
        "try:\n",
        "    _new_root_resolved = NEW_ROOT.resolve()\n",
        "    _inside_new_root = any(Path(p).resolve().is_relative_to(_new_root_resolved) for p in df[\"path\"])\n",
        "except AttributeError:\n",
        "    # for Python <3.9, emulate is_relative_to\n",
        "    def _is_rel(a, b):\n",
        "        try:\n",
        "            Path(a).resolve().relative_to(Path(b).resolve())\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "    _inside_new_root = any(_is_rel(p, NEW_ROOT) for p in df[\"path\"])\n",
        "\n",
        "if _inside_new_root:\n",
        "    print(\"[warn] Your df['path'] includes files under NEW_ROOT. \"\n",
        "          \"Youâ€™re re-splitting an already-materialized tree. \"\n",
        "          \"Weâ€™ll skip identical paths and existing files to avoid SameFileError.\")\n",
        "\n",
        "# create dirs if needed\n",
        "for sub in [\"train\", \"val\", \"test\"]:\n",
        "    for cls in CLASSES:\n",
        "        (NEW_ROOT / sub / cls).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def link_into(dst_root: Path, subset_df, subset_name: str):\n",
        "    skipped_same = skipped_exist = linked = copied = 0\n",
        "    for _, r in subset_df.iterrows():\n",
        "        src = Path(r[\"path\"]).resolve()\n",
        "        cls = \"Appendicitis\" if int(r[\"label\"]) == 1 else \"No_Appendicitis\"\n",
        "        dst = (dst_root / subset_name / cls / src.name).resolve()\n",
        "\n",
        "        # 1) skip if source == destination (self-link/copy)\n",
        "        if src == dst:\n",
        "            skipped_same += 1\n",
        "            continue\n",
        "\n",
        "        # 2) skip if destination already exists (idempotent)\n",
        "        if dst.exists():\n",
        "            skipped_exist += 1\n",
        "            continue\n",
        "\n",
        "        # 3) try hardlink, fallback to copy\n",
        "        try:\n",
        "            os.link(src, dst)  # same filesystem\n",
        "            linked += 1\n",
        "        except OSError:\n",
        "            shutil.copy2(src, dst)\n",
        "            copied += 1\n",
        "    print(f\"[{subset_name}] linked={linked} copied={copied} skipped_exist={skipped_exist} skipped_same={skipped_same}\")\n",
        "\n",
        "link_into(NEW_ROOT, train_df, \"train\")\n",
        "link_into(NEW_ROOT, val_df,   \"val\")\n",
        "link_into(NEW_ROOT, test_df,  \"test\")\n",
        "\n",
        "print(\"Built (or verified):\", NEW_ROOT.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Augment *train* only to balance classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# import albumentations as A\n",
        "# aug = A.Compose([\n",
        "#     A.HorizontalFlip(p=0.5),\n",
        "#     A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.10, rotate_limit=15,\n",
        "#                        border_mode=cv2.BORDER_REFLECT_101, p=0.8),\n",
        "#     # A.RandomResizedCrop(height=IMG_SIZE, width=IMG_SIZE, scale=(0.9,1.1), ratio=(0.95,1.05), p=0.6),\n",
        "# ])\n",
        "# tr_root = NEW_ROOT / \"train\"\n",
        "# app_dir = tr_root / \"Appendicitis\"\n",
        "# no_dir  = tr_root / \"No_Appendicitis\"\n",
        "# maj, minr = (app_dir, no_dir) if len(list(app_dir.iterdir()))>len(list(no_dir.iterdir())) else (no_dir, app_dir)\n",
        "# need = len(list(maj.iterdir())) - len(list(minr.iterdir()))\n",
        "# created = 0\n",
        "# imgs = list(minr.glob(\"*\"))\n",
        "# i = 0\n",
        "# while created < max(0, need) and imgs:\n",
        "#     p = imgs[i % len(imgs)]\n",
        "#     img = cv2.imread(str(p))\n",
        "#     if img is None: \n",
        "#         i += 1; continue\n",
        "#     out = aug(image=img)[\"image\"]\n",
        "#     out_name = minr / f\"{p.stem}_aug{created:05d}{p.suffix}\"\n",
        "#     cv2.imwrite(str(out_name), out); created += 1; i += 1\n",
        "# print(\"Augmented minority by:\", created)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train YOLOv8 classification (two-stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.3.204 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.2.103 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti Laptop GPU, 12227MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8s-cls.pt, data=datasets_70_15_15, epochs=20, time=None, patience=10, batch=32, imgsz=224, save=True, save_period=-1, cache=False, device=0, workers=4, project=runs, name=cls_s_70_15_15_e203, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.2, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.5, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/cls_s_70_15_15_e203\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/train... found 2482 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/val... found 1401 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/test... found 1445 images in 2 classes âœ… \n",
            "Overriding model.yaml nc=1000 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    660482  ultralytics.nn.modules.head.Classify         [512, 2]                      \n",
            "YOLOv8s-cls summary: 99 layers, 5,083,298 parameters, 5,083,298 gradients, 12.6 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/liori/appendicitis_project/datasets_70_15_15/train... 2482 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2482/2482 [00:05<00:00, 473.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/liori/appendicitis_project/datasets_70_15_15/train.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liori/appendicitis_project/datasets_70_15_15/val... 1401 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1401/1401 [00:02<00:00, 497.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/liori/appendicitis_project/datasets_70_15_15/val.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/cls_s_70_15_15_e203\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/20     0.757G     0.5096         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:10<00:00,  7.67it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00, 10.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.841          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/20     0.789G     0.3731         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:06<00:00, 11.35it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.852          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/20     0.812G     0.3259         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.62it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 11.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.851          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/20     0.805G     0.3063         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.89it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.867          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/20      0.81G     0.3112         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.25it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.887          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/20     0.803G     0.2896         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.61it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.888          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/20      0.81G     0.2699         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.61it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.909          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/20     0.803G     0.2603         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.56it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 12.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.911          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/20     0.803G     0.2415         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.54it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.919          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/20     0.801G     0.2348         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.42it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 13.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.934          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/20     0.803G     0.2211         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:06<00:00, 12.75it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 13.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.942          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/20     0.803G     0.2092         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.28it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.937          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/20     0.807G     0.1982         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.76it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.965          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/20     0.803G      0.188         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.66it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 13.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.963          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/20     0.803G     0.1754         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.84it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.974          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/20     0.801G     0.1599         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.54it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 13.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.974          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/20     0.803G     0.1562         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.25it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       0.98          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/20     0.803G     0.1292         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.62it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 15.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.969          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/20     0.803G     0.1297         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.80it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 13.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       0.98          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/20     0.803G     0.1249         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.99it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 13.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.982          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "20 epochs completed in 0.046 hours.\n",
            "Optimizer stripped from runs/cls_s_70_15_15_e203/weights/last.pt, 10.3MB\n",
            "Optimizer stripped from runs/cls_s_70_15_15_e203/weights/best.pt, 10.3MB\n",
            "\n",
            "Validating runs/cls_s_70_15_15_e203/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.103 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti Laptop GPU, 12227MiB)\n",
            "YOLOv8s-cls summary (fused): 73 layers, 5,077,762 parameters, 0 gradients, 12.4 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/train... found 2482 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/val... found 1401 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/test... found 1445 images in 2 classes âœ… \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 12.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.982          1\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/cls_s_70_15_15_e203\u001b[0m\n",
            "Results saved to \u001b[1mruns/cls_s_70_15_15_e203\u001b[0m\n",
            "New https://pypi.org/project/ultralytics/8.3.204 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.2.103 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti Laptop GPU, 12227MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=runs/cls_s_70_15_15_e203/weights/best.pt, data=datasets_70_15_15, epochs=15, time=None, patience=7, batch=32, imgsz=224, save=True, save_period=-1, cache=False, device=0, workers=4, project=runs, name=cls_s_70_15_15_ft153, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.0005, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/cls_s_70_15_15_ft153\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/train... found 2482 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/val... found 1401 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/test... found 1445 images in 2 classes âœ… \n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    660482  ultralytics.nn.modules.head.Classify         [512, 2]                      \n",
            "YOLOv8s-cls summary: 99 layers, 5,083,298 parameters, 5,083,298 gradients, 12.6 GFLOPs\n",
            "Transferred 158/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/liori/appendicitis_project/datasets_70_15_15/train... 2482 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2482/2482 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/liori/appendicitis_project/datasets_70_15_15/val... 1401 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1401/1401 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0005' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/cls_s_70_15_15_ft153\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/15     0.868G     0.1268         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:09<00:00,  8.61it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.981          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/15     0.895G     0.1284         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:06<00:00, 12.25it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 15.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.969          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/15      0.91G     0.1761         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.53it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 12.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.956          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/15      0.91G     0.1826         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 14.07it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.968          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/15     0.908G     0.1889         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:06<00:00, 12.26it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 14.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.974          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/15     0.908G     0.1565         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:06<00:00, 12.64it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 15.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.973          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/15      0.91G     0.1603         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.69it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 15.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.976          1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/15     0.916G     0.1164         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 13.20it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 12.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.978          1\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 7 epochs. Best results observed at epoch 1, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=7) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "8 epochs completed in 0.019 hours.\n",
            "Optimizer stripped from runs/cls_s_70_15_15_ft153/weights/last.pt, 10.3MB\n",
            "Optimizer stripped from runs/cls_s_70_15_15_ft153/weights/best.pt, 10.3MB\n",
            "\n",
            "Validating runs/cls_s_70_15_15_ft153/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.103 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti Laptop GPU, 12227MiB)\n",
            "YOLOv8s-cls summary (fused): 73 layers, 5,077,762 parameters, 0 gradients, 12.4 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/train... found 2482 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/val... found 1401 images in 2 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /home/liori/appendicitis_project/datasets_70_15_15/test... found 1445 images in 2 classes âœ… \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 13.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all      0.981          1\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/cls_s_70_15_15_ft153\u001b[0m\n",
            "Results saved to \u001b[1mruns/cls_s_70_15_15_ft153\u001b[0m\n",
            "Best weights: runs/cls_s_70_15_15_ft153/weights/best.pt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = str(NEW_ROOT)\n",
        "model = YOLO(\"yolov8s-cls.pt\")\n",
        "\n",
        "res1 = model.train(\n",
        "    data=DATA_DIR, epochs=EPOCHS_STAGE1, imgsz=IMG_SIZE, batch=BATCH, device=DEVICE,\n",
        "    amp=True, workers=4, auto_augment=\"randaugment\", erasing=0.5, dropout=0.2,\n",
        "    patience=10, project=\"runs\", name=f\"{RUN_NAME_BASE}_e{EPOCHS_STAGE1}\"\n",
        ")\n",
        "best1 = Path(res1.save_dir)/\"weights\"/\"best.pt\"\n",
        "\n",
        "res2 = YOLO(str(best1)).train(\n",
        "    data=DATA_DIR, epochs=EPOCHS_STAGE2, imgsz=IMG_SIZE, batch=BATCH, device=DEVICE,\n",
        "    amp=True, workers=4, lr0=5e-4, patience=7, project=\"runs\", name=f\"{RUN_NAME_BASE}_ft{EPOCHS_STAGE2}\"\n",
        ")\n",
        "BEST_WEIGHTS = str(Path(res2.save_dir)/\"weights\"/\"best.pt\")\n",
        "print(\"Best weights:\", BEST_WEIGHTS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3c669e88",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "â€”â€” Training summary â€”â€”\n",
            "Stage 1: runs/cls_s_70_15_15_e203\n",
            "  epochs=20 | best_epoch=19 | top1=0.9822 | top5=1.0000 | val_loss=0.3396\n",
            "Stage 2: runs/cls_s_70_15_15_ft153\n",
            "  epochs=8 | best_epoch=0 | top1=0.9807 | top5=1.0000 | val_loss=0.3420\n",
            "\n",
            "Best weights: runs/cls_s_70_15_15_ft153/weights/best.pt\n",
            "\n",
            "Dataset summary:\n",
            "  train: {'Appendicitis': 1336, 'No_Appendicitis': 1146} | total=2482\n",
            "  val: {'Appendicitis': 744, 'No_Appendicitis': 657} | total=1401\n",
            "  test: {'Appendicitis': 782, 'No_Appendicitis': 663} | total=1445\n"
          ]
        }
      ],
      "source": [
        "# --- Training summary (stage 1 + stage 2) ---\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def _find_col(df, keys):\n",
        "    for k in keys:\n",
        "        hits = [c for c in df.columns if k in c]\n",
        "        if hits:\n",
        "            return hits[0]\n",
        "    return None\n",
        "\n",
        "def summarize_run(run_dir: Path):\n",
        "    csv = Path(run_dir) / \"results.csv\"\n",
        "    if not csv.exists():\n",
        "        return None\n",
        "    df = pd.read_csv(csv)\n",
        "\n",
        "    # likely column names for YOLOv8-cls\n",
        "    top1  = _find_col(df, [\"metrics/accuracy_top1\", \"top1\"])\n",
        "    top5  = _find_col(df, [\"metrics/accuracy_top5\", \"top5\"])\n",
        "    vloss = _find_col(df, [\"val/loss\", \"val_loss\"])\n",
        "    tloss = _find_col(df, [\"train/loss\", \"loss\"])\n",
        "\n",
        "    if top1 and top1 in df:\n",
        "        best_idx = int(df[top1].idxmax())\n",
        "    elif vloss and vloss in df:\n",
        "        best_idx = int(df[vloss].idxmin())\n",
        "    else:\n",
        "        best_idx = len(df) - 1\n",
        "\n",
        "    best = df.loc[best_idx]\n",
        "    last = df.iloc[-1]\n",
        "\n",
        "    out = {\n",
        "        \"epochs_total\": int(df[\"epoch\"].iloc[-1]) + 1 if \"epoch\" in df else len(df),\n",
        "        \"best_epoch\":   int(best[\"epoch\"]) if \"epoch\" in df else best_idx,\n",
        "        \"best_top1\":    float(best[top1]) if top1 and top1 in df else None,\n",
        "        \"best_top5\":    float(best[top5]) if top5 and top5 in df else None,\n",
        "        \"best_val_loss\":float(best[vloss]) if vloss and vloss in df else None,\n",
        "        \"last_top1\":    float(last[top1]) if top1 and top1 in df else None,\n",
        "        \"last_val_loss\":float(last[vloss]) if vloss and vloss in df else None,\n",
        "        \"csv_path\":     str(csv),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "s1 = summarize_run(res1.save_dir)\n",
        "s2 = summarize_run(res2.save_dir)\n",
        "\n",
        "print(\"\\nâ€”â€” Training summary â€”â€”\")\n",
        "print(f\"Stage 1: {res1.save_dir}\")\n",
        "if s1:\n",
        "    print(f\"  epochs={s1['epochs_total']} | best_epoch={s1['best_epoch']} \"\n",
        "          f\"| top1={s1['best_top1']:.4f} \"\n",
        "          f\"{'' if s1['best_top5'] is None else f'| top5={s1['best_top5']:.4f}'} \"\n",
        "          f\"{'' if s1['best_val_loss'] is None else f'| val_loss={s1['best_val_loss']:.4f}'}\")\n",
        "else:\n",
        "    print(\"  (no results.csv)\")\n",
        "\n",
        "print(f\"Stage 2: {res2.save_dir}\")\n",
        "if s2:\n",
        "    print(f\"  epochs={s2['epochs_total']} | best_epoch={s2['best_epoch']} \"\n",
        "          f\"| top1={s2['best_top1']:.4f} \"\n",
        "          f\"{'' if s2['best_top5'] is None else f'| top5={s2['best_top5']:.4f}'} \"\n",
        "          f\"{'' if s2['best_val_loss'] is None else f'| val_loss={s2['best_val_loss']:.4f}'}\")\n",
        "else:\n",
        "    print(\"  (no results.csv)\")\n",
        "\n",
        "print(f\"\\nBest weights: {BEST_WEIGHTS}\")\n",
        "\n",
        "# (bonus) show dataset counts used\n",
        "from pathlib import Path\n",
        "def split_counts(root, split, classes=(\"Appendicitis\",\"No_Appendicitis\")):\n",
        "    root = Path(root) / split\n",
        "    if not root.exists(): \n",
        "        return {}\n",
        "    return {c: len(list((root/c).glob(\"*\"))) for c in classes}\n",
        "\n",
        "print(\"\\nDataset summary:\")\n",
        "for split in (\"train\",\"val\",\"test\"):\n",
        "    cnt = split_counts(DATA_DIR, split)\n",
        "    if cnt:\n",
        "        print(f\"  {split}: {cnt} | total={sum(cnt.values())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7e0213ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[scan weights] found: ['runs/cls_s_70_15_15_ft153/weights/best.pt', 'runs/cls_s_70_15_15_e203/weights/best.pt', 'runs/cls_s_70_15_15_ft152/weights/best.pt', 'runs/cls_s_70_15_15_e202/weights/best.pt', 'runs/cls_s_70_15_15_ft15/weights/best.pt']\n",
            "[use weights] runs/cls_s_70_15_15_ft153/weights/best.pt exists: True\n",
            "[sample img] datasets_70_15_15/val/Appendicitis/505.2 App_M.bmp\n",
            "[model task?] names type: <class 'dict'> | len: 2\n",
            "\n",
            "image 1/1 /home/liori/appendicitis_project/datasets_70_15_15/val/Appendicitis/505.2 App_M.bmp: 224x224 Appendicitis 0.91, No_Appendicitis 0.09, 5.2ms\n",
            "[result attrs] has probs: True | probs type: <class 'ultralytics.engine.results.Probs'>\n",
            "[probs len] 2 | first 5: ultralytics.engine.results.Probs object with attributes:\n",
            "\n",
            "data: tensor([0.9106, 0.0894], device='cuda:0')\n",
            "orig_shape: None\n",
            "shape: torch.Size([2])\n",
            "top1: 0\n",
            "top1conf: tensor(0.9106, device='cuda:0')\n",
            "top5: [0, 1]\n",
            "top5conf: tensor([0.9106, 0.0894], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import glob, os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "SPLIT    = \"val\"         # or \"test\"\n",
        "DATA_DIR = \"datasets_70_15_15\"\n",
        "\n",
        "# 1) Find a plausible best.pt automatically if BEST_WEIGHTS is missing\n",
        "try:\n",
        "    WEIGHTS\n",
        "except NameError:\n",
        "    cands = sorted(glob.glob(\"runs/**/weights/best.pt\", recursive=True), key=os.path.getmtime, reverse=True)\n",
        "    print(\"[scan weights] found:\", cands[:5])\n",
        "    assert cands, \"No best.pt found under runs/**/weights/. Train cell not run yet?\"\n",
        "    WEIGHTS = cands[0]\n",
        "\n",
        "print(\"[use weights]\", WEIGHTS, \"exists:\", Path(WEIGHTS).exists())\n",
        "\n",
        "# 2) Grab one image to test\n",
        "from pathlib import Path\n",
        "CLASSES = [\"Appendicitis\",\"No_Appendicitis\"]\n",
        "ALLOWED = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
        "\n",
        "sample = None\n",
        "for cls in CLASSES:\n",
        "    d = Path(DATA_DIR)/SPLIT/cls\n",
        "    if d.is_dir():\n",
        "        pics = [f for f in d.glob(\"*\") if f.suffix.lower() in ALLOWED]\n",
        "        if pics:\n",
        "            sample = str(pics[0]); break\n",
        "print(\"[sample img]\", sample)\n",
        "\n",
        "# 3) Inspect the model and a single prediction\n",
        "m = YOLO(WEIGHTS)\n",
        "print(\"[model task?] names type:\", type(m.names), \"| len:\", (len(m.names) if hasattr(m, \"names\") else None))\n",
        "try:\n",
        "    res = next(m.predict(sample, stream=True, verbose=True))\n",
        "    print(\"[result attrs] has probs:\", hasattr(res, \"probs\"), \"| probs type:\", type(getattr(res, \"probs\", None)))\n",
        "    if hasattr(res, \"probs\") and res.probs is not None:\n",
        "        print(\"[probs len]\", len(res.probs), \"| first 5:\", res.probs[:5])\n",
        "    else:\n",
        "        print(\"[warn] res.probs is None -> this weights file is likely NOT a classification model\")\n",
        "except Exception as e:\n",
        "    print(\"[predict error]\", repr(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "041cf7b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[scan] val: {'Appendicitis': 744, 'No_Appendicitis': 657} | readable=1401\n",
            "[model] Appendicitis class index: 0\n",
            "[predict] kept=1401  failed=0\n",
            "[final] usable=1401 | positives=744 | negatives=657\n",
            "val: Ï„*=0.700 | No recall=0.993 | App recall=0.991\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === One-shot: rebuild + sweep (per-image, no stream, bulletproof) ===\n",
        "import os, json, numpy as np, cv2, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# ---- set these for your run ----\n",
        "SPLIT    = \"val\"  # or \"test\"\n",
        "DATA_DIR = \"datasets_70_15_15\"\n",
        "WEIGHTS  = \"runs/cls_s_70_15_15_ft152/weights/best.pt\"  # your confirmed cls weights\n",
        "DEVICE   = 0   # set to 'cpu' if GPU is flaky\n",
        "IMG_SIZE = 224\n",
        "CLASSES  = [\"Appendicitis\", \"No_Appendicitis\"]\n",
        "ALLOWED  = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
        "def nrm(s): return s.lower().replace(\"_\",\"\").replace(\" \",\"\")\n",
        "\n",
        "# 0) clear stale caches for this split\n",
        "for p in [f\"{SPLIT}_scores_appendicitis.npy\", f\"{SPLIT}_y_true.npy\"]:\n",
        "    try: Path(p).unlink()\n",
        "    except FileNotFoundError: pass\n",
        "\n",
        "# 1) enumerate readable files (absolute paths)\n",
        "root = Path(DATA_DIR)\n",
        "assert (root/SPLIT).is_dir(), f\"Missing split dir: {root/SPLIT}\"\n",
        "rows = []\n",
        "counts = {}\n",
        "for cls in CLASSES:\n",
        "    d = root / SPLIT / cls\n",
        "    assert d.is_dir(), f\"Missing class dir: {d}\"\n",
        "    files = [f for f in d.glob(\"*\") if f.suffix.lower() in ALLOWED]\n",
        "    counts[cls] = len(files)\n",
        "    lab = 1 if nrm(cls)==\"appendicitis\" else 0\n",
        "    for f in files:\n",
        "        pabs = f.resolve()\n",
        "        img = cv2.imread(str(pabs), cv2.IMREAD_UNCHANGED)\n",
        "        if img is not None:\n",
        "            rows.append((str(pabs), lab))\n",
        "print(f\"[scan] {SPLIT}: {counts} | readable={len(rows)}\")\n",
        "assert rows, \"No readable images.\"\n",
        "\n",
        "# 2) init model and resolve class index once\n",
        "model = YOLO(WEIGHTS)\n",
        "names = getattr(model, \"names\", None)\n",
        "if isinstance(names, dict):\n",
        "    name_map = {i: nrm(v) for i, v in names.items()}\n",
        "    app_idx = [k for k,v in name_map.items() if v==\"appendicitis\"]\n",
        "    app_idx = app_idx[0] if app_idx else 0\n",
        "else:\n",
        "    app_idx = 0\n",
        "print(f\"[model] Appendicitis class index: {app_idx}\")\n",
        "\n",
        "# 3) predict one-by-one (no stream), collect aligned pairs\n",
        "pairs = []   # (score_app, label)\n",
        "fails = 0\n",
        "examples_logged = 0\n",
        "\n",
        "for pth, lab in rows:\n",
        "    try:\n",
        "        res_list = model.predict(pth, imgsz=IMG_SIZE, device=DEVICE, verbose=False)  # <- no stream\n",
        "        if not res_list or len(res_list) == 0:\n",
        "            fails += 1\n",
        "            if examples_logged < 5:\n",
        "                print(\"[fail] empty result for:\", pth)\n",
        "                examples_logged += 1\n",
        "            continue\n",
        "        res = res_list[0]\n",
        "        # Some builds require .data; standardize to scalar\n",
        "        score = res.probs[app_idx]\n",
        "        try:\n",
        "            score = float(score)\n",
        "        except Exception:\n",
        "            # try .data path\n",
        "            score = float(getattr(res.probs, \"data\")[app_idx].item())\n",
        "        if np.isfinite(score):\n",
        "            pairs.append((score, lab))\n",
        "        else:\n",
        "            fails += 1\n",
        "            if examples_logged < 5:\n",
        "                print(\"[fail] non-finite score for:\", pth)\n",
        "                examples_logged += 1\n",
        "    except Exception as e:\n",
        "        fails += 1\n",
        "        if examples_logged < 5:\n",
        "            print(\"[fail] predict error:\", pth, \"|\", repr(e))\n",
        "            examples_logged += 1\n",
        "\n",
        "print(f\"[predict] kept={len(pairs)}  failed={fails}\")\n",
        "assert len(pairs) > 0, \"No usable predictions. If this persists, set DEVICE='cpu' and re-run.\"\n",
        "\n",
        "# 4) unpack pairs â†’ aligned arrays\n",
        "scores = np.array([s for s, _ in pairs], dtype=float)\n",
        "y_true = np.array([y for _, y in pairs], dtype=int).tolist()\n",
        "print(f\"[final] usable={len(scores)} | positives={sum(y_true)} | negatives={len(y_true)-sum(y_true)}\")\n",
        "\n",
        "# 5) save caches\n",
        "np.save(f\"{SPLIT}_scores_appendicitis.npy\", scores)\n",
        "np.save(f\"{SPLIT}_y_true.npy\", np.array(y_true, dtype=int))\n",
        "\n",
        "# 6) sweep threshold with guardrail\n",
        "grid = np.arange(0.70, 0.90, 0.0025)\n",
        "best = None\n",
        "for t in grid:\n",
        "    y_pred = [1 if s >= t else 0 for s in scores]  # 1=Appendicitis, 0=No\n",
        "    rpt = classification_report(\n",
        "        y_true, y_pred,\n",
        "        labels=[0,1], target_names=CLASSES,\n",
        "        output_dict=True, zero_division=0\n",
        "    )\n",
        "    app_rec = rpt[\"Appendicitis\"][\"recall\"]; no_rec = rpt[\"No_Appendicitis\"][\"recall\"]\n",
        "    if app_rec >= 0.90 and (best is None or no_rec > best[0]):\n",
        "        best = (no_rec, float(t), rpt, y_pred)\n",
        "\n",
        "if best is None:\n",
        "    for t in grid:\n",
        "        y_pred = [1 if s >= t else 0 for s in scores]\n",
        "        rpt = classification_report(\n",
        "            y_true, y_pred, labels=[0,1], target_names=CLASSES,\n",
        "            output_dict=True, zero_division=0\n",
        "        )\n",
        "        no_rec = rpt[\"No_Appendicitis\"][\"recall\"]\n",
        "        if (best is None) or (no_rec > best[0]):\n",
        "            best = (no_rec, float(t), rpt, y_pred)\n",
        "\n",
        "BEST_TAU, rpt_best, y_pred_best = best[1], best[2], best[3]\n",
        "print(f\"{SPLIT}: Ï„*={BEST_TAU:.3f} | No recall={rpt_best['No_Appendicitis']['recall']:.3f} | App recall={rpt_best['Appendicitis']['recall']:.3f}\")\n",
        "\n",
        "Path(\"inference_config.json\").write_text(json.dumps({\"BEST_TAU\": float(BEST_TAU)}, indent=2))\n",
        "cm = confusion_matrix(y_true, y_pred_best, labels=[0,1])\n",
        "ConfusionMatrixDisplay(cm, display_labels=CLASSES).plot(values_format='d', colorbar=True)\n",
        "plt.title(f\"{SPLIT.upper()} Confusion Matrix @ Ï„={BEST_TAU:.3f}\")\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare scores/y_true for a chosen split (val or test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[prep] kept=1401  failed=0\n",
            "Saved val_scores_appendicitis.npy / val_y_true.npy | n=1401\n"
          ]
        }
      ],
      "source": [
        "# === Prepare scores/y_true without stream (robust) ===\n",
        "import numpy as np, cv2, pandas as pd\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# --- set these explicitly ---\n",
        "SPLIT    = \"val\"  # or \"test\"\n",
        "DATA_DIR = \"datasets_70_15_15\"\n",
        "WEIGHTS  = \"runs/cls_s_70_15_15_ft152/weights/best.pt\"  # <- use the path you just confirmed works\n",
        "DEVICE   = 0      # or 'cpu' if GPU is cranky\n",
        "IMG_SIZE = 224\n",
        "CLASSES  = [\"Appendicitis\", \"No_Appendicitis\"]\n",
        "ALLOWED  = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
        "def nrm(s): return s.lower().replace(\"_\",\"\").replace(\" \",\"\")\n",
        "# ----------------------------\n",
        "\n",
        "root = Path(DATA_DIR)\n",
        "assert (root/SPLIT).is_dir(), f\"Missing split dir: {root/SPLIT}\"\n",
        "\n",
        "# collect readable files with labels\n",
        "rows = []\n",
        "for cls in CLASSES:\n",
        "    d = root / SPLIT / cls\n",
        "    assert d.is_dir(), f\"Missing class dir: {d}\"\n",
        "    files = [f for f in d.glob(\"*\") if f.suffix.lower() in ALLOWED]\n",
        "    lab = 1 if nrm(cls) == \"appendicitis\" else 0\n",
        "    for f in files:\n",
        "        img = cv2.imread(str(f), cv2.IMREAD_UNCHANGED)\n",
        "        if img is not None:\n",
        "            rows.append((str(f.resolve()), lab))\n",
        "assert rows, \"No readable images.\"\n",
        "\n",
        "# init model and resolve Appendicitis index once\n",
        "model = YOLO(WEIGHTS)\n",
        "names = getattr(model, \"names\", None)\n",
        "if isinstance(names, dict):\n",
        "    app_idx = [i for i, v in names.items() if nrm(v) == \"appendicitis\"]\n",
        "    app_idx = app_idx[0] if app_idx else 0\n",
        "else:\n",
        "    app_idx = 0\n",
        "\n",
        "# predict one-by-one (no stream); collect aligned (score,label) pairs\n",
        "pairs, fails = [], 0\n",
        "for pth, lab in rows:\n",
        "    try:\n",
        "        res_list = model.predict(pth, imgsz=IMG_SIZE, device=DEVICE, verbose=False)  # NO stream\n",
        "        if not res_list:\n",
        "            fails += 1; continue\n",
        "        res = res_list[0]\n",
        "        # standardize to float\n",
        "        score = res.probs[app_idx]\n",
        "        try:\n",
        "            score = float(score)\n",
        "        except Exception:\n",
        "            score = float(getattr(res.probs, \"data\")[app_idx].item())\n",
        "        if np.isfinite(score):\n",
        "            pairs.append((score, lab))\n",
        "        else:\n",
        "            fails += 1\n",
        "    except Exception:\n",
        "        fails += 1\n",
        "\n",
        "print(f\"[prep] kept={len(pairs)}  failed={fails}\")\n",
        "assert pairs, \"No usable predictions. If needed, set DEVICE='cpu' and rerun.\"\n",
        "\n",
        "# unpack to aligned arrays and save\n",
        "scores = np.array([s for s, _ in pairs], float)\n",
        "y_true = np.array([y for _, y in pairs], int)\n",
        "np.save(f\"{SPLIT}_scores_appendicitis.npy\", scores)\n",
        "np.save(f\"{SPLIT}_y_true.npy\", y_true)\n",
        "print(f\"Saved {SPLIT}_scores_appendicitis.npy / {SPLIT}_y_true.npy | n={len(scores)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold sweep (split-aware, guardrail on Appendicitis recall â‰¥ 0.90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: 1401 1401\n",
            "val: Ï„*=0.700 | No recall=0.993 | App recall=0.991\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import numpy as np, json\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SPLIT = \"val\"\n",
        "CLASSES = [\"Appendicitis\", \"No_Appendicitis\"]\n",
        "\n",
        "scores = np.load(f\"{SPLIT}_scores_appendicitis.npy\")\n",
        "y_true = np.load(f\"{SPLIT}_y_true.npy\").astype(int).tolist()\n",
        "\n",
        "print(\"Loaded:\", len(scores), len(y_true))\n",
        "assert len(scores)==len(y_true) and len(scores)>0\n",
        "\n",
        "grid = np.arange(0.70, 0.90, 0.0025)\n",
        "best = None\n",
        "for t in grid:\n",
        "    y_pred = [1 if s >= t else 0 for s in scores]  # 1=App, 0=No\n",
        "    rpt = classification_report(\n",
        "        y_true, y_pred,\n",
        "        labels=[0,1], target_names=CLASSES,\n",
        "        output_dict=True, zero_division=0\n",
        "    )\n",
        "    app_rec = rpt[\"Appendicitis\"][\"recall\"]; no_rec = rpt[\"No_Appendicitis\"][\"recall\"]\n",
        "    if app_rec >= 0.90 and (best is None or no_rec > best[0]):\n",
        "        best = (no_rec, float(t), rpt, y_pred)\n",
        "\n",
        "if best is None:\n",
        "    for t in grid:\n",
        "        y_pred = [1 if s >= t else 0 for s in scores]\n",
        "        rpt = classification_report(\n",
        "            y_true, y_pred, labels=[0,1], target_names=CLASSES,\n",
        "            output_dict=True, zero_division=0\n",
        "        )\n",
        "        no_rec = rpt[\"No_Appendicitis\"][\"recall\"]\n",
        "        if (best is None) or (no_rec > best[0]):\n",
        "            best = (no_rec, float(t), rpt, y_pred)\n",
        "\n",
        "BEST_TAU, rpt_best, y_pred_best = best[1], best[2], best[3]\n",
        "print(f\"{SPLIT}: Ï„*={BEST_TAU:.3f} | No recall={rpt_best['No_Appendicitis']['recall']:.3f} | App recall={rpt_best['Appendicitis']['recall']:.3f}\")\n",
        "\n",
        "Path(\"inference_config.json\").write_text(json.dumps({\"BEST_TAU\": float(BEST_TAU)}, indent=2))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_best, labels=[0,1])\n",
        "ConfusionMatrixDisplay(cm, display_labels=CLASSES).plot(values_format='d', colorbar=True)\n",
        "plt.title(f\"{SPLIT.upper()} Confusion Matrix @ Ï„={BEST_TAU:.3f}\")\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72ba7cd",
      "metadata": {},
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8ae95801",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "   Appendicitis       0.99      0.99      0.99       744\n",
            "No_Appendicitis       0.99      0.99      0.99       657\n",
            "\n",
            "       accuracy                           0.99      1401\n",
            "      macro avg       0.99      0.99      0.99      1401\n",
            "   weighted avg       0.99      0.99      0.99      1401\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 550x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === Show confusion matrix for a split at a fixed Ï„ (no helpers) ===\n",
        "%matplotlib inline\n",
        "import json, numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SPLIT   = \"val\"  # or \"test\"\n",
        "CLASSES = [\"Appendicitis\",\"No_Appendicitis\"]\n",
        "\n",
        "# load tau chosen on VAL\n",
        "BEST_TAU = json.loads(Path(\"inference_config.json\").read_text())[\"BEST_TAU\"]\n",
        "\n",
        "# load arrays for the chosen split (make sure you've run the prep cell for this split)\n",
        "scores = np.load(f\"{SPLIT}_scores_appendicitis.npy\")\n",
        "y_true = np.load(f\"{SPLIT}_y_true.npy\").astype(int)\n",
        "\n",
        "# binarize and print report\n",
        "y_pred = (scores >= BEST_TAU).astype(int)\n",
        "print(\n",
        "    classification_report(\n",
        "        y_true, y_pred, labels=[1,0], target_names=CLASSES, digits=2, zero_division=0\n",
        "    )\n",
        ")\n",
        "\n",
        "# draw confusion matrix inline\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=CLASSES)\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5))\n",
        "disp.plot(ax=ax, values_format=\"d\", colorbar=True)\n",
        "ax.set_title(f\"{SPLIT.upper()} Confusion Matrix @ Ï„={BEST_TAU:.3f}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d8d3950c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accuracy': 0.9911, 'recall_App': 0.9934, 'recall_No': 0.9885, 'precision_App': 0.9901, 'precision_No': 0.9923}\n"
          ]
        }
      ],
      "source": [
        "# sanity-check from the confusion matrix counts\n",
        "TP_app, FN_app = 601, 4\n",
        "FP_app, TN_app = 6, 517  # FP_app is Noâ†’App; TN_app is Noâ†’No\n",
        "total = TP_app + FN_app + FP_app + TN_app\n",
        "\n",
        "acc = (TP_app + TN_app) / total\n",
        "rec_app = TP_app / (TP_app + FN_app)\n",
        "rec_no  = TN_app / (TN_app + FP_app)\n",
        "prec_app = TP_app / (TP_app + FP_app)\n",
        "prec_no  = TN_app / (TN_app + FN_app)\n",
        "\n",
        "print({\n",
        "    \"accuracy\": round(acc, 4),\n",
        "    \"recall_App\": round(rec_app, 4),\n",
        "    \"recall_No\": round(rec_no, 4),\n",
        "    \"precision_App\": round(prec_app, 4),\n",
        "    \"precision_No\": round(prec_no, 4),\n",
        "})\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
